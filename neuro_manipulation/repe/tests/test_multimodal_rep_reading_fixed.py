#!/usr/bin/env python3
"""
Fixed unit tests for multimodal representation reading functionality.
Uses direct method testing instead of full Transformers Pipeline mocking.
"""

import pytest
import torch
from PIL import Image
import numpy as np
from unittest.mock import Mock, patch
import sys
import os

# Add project root to path
sys.path.append('/data/home/jjl7137/LLM_EmoBehav_game_theory_multimodal')

from neuro_manipulation.repe.rep_reading_pipeline import RepReadingPipeline
from neuro_manipulation.model_layer_detector import ModelLayerDetector
from neuro_manipulation.prompt_formats import QwenVLInstFormat, ManualPromptFormat


class TestMultimodalFunctionality:
    """Test multimodal functionality without full pipeline mocking."""
    
    def test_multimodal_input_detection(self):
        """Test multimodal input detection logic."""
        # Create pipeline instance for testing methods
        pipeline = RepReadingPipeline.__new__(RepReadingPipeline)
        
        sample_image = Image.new('RGB', (224, 224), color='red')
        
        # Test multimodal input detection
        multimodal_input = {
            'images': [sample_image],
            'text': 'when you see this image, your emotion is anger'
        }
        
        is_multimodal = pipeline._is_multimodal_input(multimodal_input)
        assert is_multimodal, "Should detect multimodal input"
        
        # Test text-only input
        text_input = "This is just text"
        is_text_only = pipeline._is_multimodal_input(text_input)
        assert not is_text_only, "Should not detect text-only as multimodal"
        
        # Test singular image input
        singular_input = {
            'image': sample_image,
            'text': 'when you see this image, your emotion is happiness'
        }
        is_singular = pipeline._is_multimodal_input(singular_input)
        assert is_singular, "Should detect singular image input as multimodal"
    
    def test_prompt_format_integration(self):
        """Test prompt format integration with RepE pipeline."""
        # Mock tokenizer with Qwen-VL name
        mock_tokenizer = Mock()
        mock_tokenizer.name_or_path = "Qwen2.5-VL-3B-Instruct"
        mock_tokenizer.special_tokens_map = {
            'additional_special_tokens': ['<|vision_start|>', '<|vision_end|>', '<|image_pad|>']
        }
        mock_tokenizer.added_tokens_decoder = {
            151652: Mock(content='<|vision_start|>'),
            151653: Mock(content='<|vision_end|>'),
            151655: Mock(content='<|image_pad|>')
        }
        # Mock tokenizer to return proper dict
        mock_tokenizer_return = {
            'input_ids': torch.tensor([[151652, 1234, 5678, 151653]]),
            'attention_mask': torch.tensor([[1, 1, 1, 1]])
        }
        mock_tokenizer.return_value = mock_tokenizer_return
        
        # Mock image processor that returns proper dict
        mock_processor_return = {
            'input_ids': torch.tensor([[151652, 1234, 5678, 151653]]),
            'pixel_values': torch.randn(1, 3, 224, 224),
            'attention_mask': torch.tensor([[1, 1, 1, 1]])
        }
        mock_processor = Mock()
        mock_processor.return_value = mock_processor_return
        mock_processor.preprocess = Mock(return_value=mock_processor_return)
        
        # Create pipeline instance
        pipeline = RepReadingPipeline.__new__(RepReadingPipeline)
        pipeline.tokenizer = mock_tokenizer
        pipeline.image_processor = mock_processor
        
        # Test input preparation
        sample_image = Image.new('RGB', (224, 224), color='blue')
        multimodal_input = {
            'images': [sample_image],
            'text': 'when you see this image, your emotion is anger'
        }
        
        # Mock the tokenizer's apply_chat_template method
        mock_tokenizer.apply_chat_template = Mock(return_value="<|im_start|>user\nwhen you see this image, your emotion is anger<|im_end|>\n<|im_start|>assistant\n")
        
        # Test input preparation with new direct approach
        result = pipeline._prepare_multimodal_inputs(multimodal_input)
        
        # Verify tokenizer's apply_chat_template was called
        mock_tokenizer.apply_chat_template.assert_called_once()
        call_args = mock_tokenizer.apply_chat_template.call_args
        # Should be called with messages, tokenize=False, add_generation_prompt=True
        assert call_args[1]['tokenize'] == False
        assert call_args[1]['add_generation_prompt'] == True
        
        # Verify processor was called with formatted text and images
        mock_processor.assert_called_once()
        call_args = mock_processor.call_args
        # Should be called with text, images, padding, return_tensors
        assert 'text' in call_args[1]
        assert 'images' in call_args[1]
        assert 'padding' in call_args[1]
        assert 'return_tensors' in call_args[1]
    
    def test_model_layer_detection(self):
        """Test model layer detection for multimodal models."""
        # Test multimodal detection by name only
        mock_model = Mock()
        mock_model.__class__.__name__ = "Qwen2VLForConditionalGeneration"
        
        # Test multimodal detection by name
        is_multimodal = ModelLayerDetector.is_multimodal_model(mock_model)
        assert is_multimodal, "Should detect Qwen model as multimodal"
        
        # Test that layer info method exists and returns dict structure
        # Skip the actual ModuleList testing to avoid PyTorch type checking issues
        mock_model.named_modules.return_value = []  # Empty modules to avoid isinstance checks
        
        layer_info = ModelLayerDetector.get_multimodal_layer_info(mock_model)
        assert isinstance(layer_info, dict), "Should return dict structure"
        assert 'vision_layers' in layer_info
        assert 'text_layers' in layer_info
        assert 'fusion_layers' in layer_info
        assert 'cross_attention_layers' in layer_info
    
    def test_qwen_vl_format_functionality(self):
        """Test QwenVL format class functionality."""
        # Test name pattern
        assert QwenVLInstFormat.name_pattern("Qwen2.5-VL-3B-Instruct")
        assert QwenVLInstFormat.name_pattern("qwen-vl-chat")
        assert not QwenVLInstFormat.name_pattern("llama-3.1-instruct")
        
        # Test multimodal support
        assert QwenVLInstFormat.supports_multimodal()
        
        # Test text-only formatting
        text_only = QwenVLInstFormat.build(
            system_prompt=None,
            user_messages=["What is the weather?"],
            assistant_answers=[]
        )
        expected_text = '<|im_start|>user\nWhat is the weather?<|im_end|>\n<|im_start|>assistant\n'
        assert text_only == expected_text
        
        # Test multimodal formatting
        sample_image = Image.new('RGB', (224, 224), 'red')
        multimodal = QwenVLInstFormat.build(
            system_prompt=None,
            user_messages=["when you see this image, your emotion is anger"],
            assistant_answers=[],
            images=[sample_image]
        )
        expected_multimodal = '<|im_start|>user\nwhen you see this image, your emotion is anger<|vision_start|><|image_pad|><|vision_end|><|im_end|>\n<|im_start|>assistant\n'
        assert multimodal == expected_multimodal
    
    def test_tokenizer_validation(self):
        """Test tokenizer validation logic."""
        # Mock tokenizer with required tokens
        good_tokenizer = Mock()
        good_tokenizer.special_tokens_map = {
            'additional_special_tokens': ['<|vision_start|>', '<|vision_end|>', '<|image_pad|>']
        }
        good_tokenizer.added_tokens_decoder = {
            151652: Mock(content='<|vision_start|>'),
            151653: Mock(content='<|vision_end|>'),
            151655: Mock(content='<|image_pad|>')
        }
        
        assert QwenVLInstFormat.validate_tokenizer(good_tokenizer)
        
        # Mock tokenizer missing tokens
        bad_tokenizer = Mock()
        bad_tokenizer.special_tokens_map = {'additional_special_tokens': []}
        bad_tokenizer.added_tokens_decoder = {}
        
        assert not QwenVLInstFormat.validate_tokenizer(bad_tokenizer)
    
    def test_manual_format_selection(self):
        """Test manual format selection works."""
        # Test QwenVL selection
        qwen_format = ManualPromptFormat.get("Qwen2.5-VL-3B-Instruct")
        assert qwen_format == QwenVLInstFormat
        
        # Test fallback for unsupported models
        with pytest.raises(ValueError):
            ManualPromptFormat.get("unknown-model")
    
    def test_batch_input_detection(self):
        """Test batch multimodal input detection."""
        pipeline = RepReadingPipeline.__new__(RepReadingPipeline)
        
        sample_image = Image.new('RGB', (224, 224), 'blue')
        batch_inputs = [
            {'images': [sample_image], 'text': 'emotion is anger'},
            {'images': [sample_image], 'text': 'emotion is happiness'}
        ]
        
        is_multimodal = pipeline._is_multimodal_input(batch_inputs)
        assert is_multimodal, "Should detect batch multimodal input"


class TestConfigurationCompatibility:
    """Test configuration file compatibility."""
    
    def test_multimodal_config_structure(self):
        """Test multimodal configuration file structure."""
        config_path = "/data/home/jjl7137/LLM_EmoBehav_game_theory_multimodal/config/multimodal_rep_reading_config.yaml"
        
        if not os.path.exists(config_path):
            pytest.skip("Config file not found")
        
        import yaml
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Check required sections
        assert 'experiment' in config
        assert 'pipeline' in config['experiment']
        assert 'emotions' in config['experiment']
        
        # Validate pipeline config
        pipeline_config = config['experiment']['pipeline']
        assert pipeline_config['task'] == 'multimodal-rep-reading'
        assert pipeline_config['rep_token'] == -1
        assert 'hidden_layers' in pipeline_config
        
        # Validate emotions
        emotions = config['experiment']['emotions']
        required_emotions = ['anger', 'happiness', 'sadness', 'disgust', 'fear', 'surprise']
        for emotion in required_emotions:
            assert emotion in emotions


def run_fixed_tests():
    """Run the fixed unit tests."""
    print("ðŸ”§ Running Fixed Multimodal Unit Tests")
    print("=" * 60)
    
    # Run with pytest
    result = pytest.main([
        __file__,
        "-v",
        "-s",
        "--tb=short"
    ])
    
    return result == 0


if __name__ == "__main__":
    success = run_fixed_tests()
    sys.exit(0 if success else 1)