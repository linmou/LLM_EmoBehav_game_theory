# Emotion Memory Experiments: Data Flow Architecture

## Overview

This document describes the complete data flow architecture for emotion memory experiments, with special focus on the relationship between PromptFormat and PromptWrapper components.

## Core Architecture Components

### 1. Model Setup & PromptFormat Creation

```
setup_model_and_tokenizer(repe_config, from_vllm=False)
    ‚Üì
Creates model-specific PromptFormat instance:
    - Qwen2.5 ‚Üí Qwen2.5PromptFormat  
    - Llama ‚Üí LlamaPromptFormat
    - GPT ‚Üí OpenAIPromptFormat
    - Other ‚Üí BasePromptFormat
    ‚Üì
Returns: (model, tokenizer, prompt_format, processor)
```

**Key Point**: Each model gets its own PromptFormat class that knows how to apply that model's specific chat template and tokenization requirements.

### 2. PromptWrapper Creation & Integration

```
get_memory_prompt_wrapper(task_type, prompt_format)
    ‚Üì
Factory creates task-specific wrapper:
    - "passkey" ‚Üí PasskeyPromptWrapper(prompt_format)
    - "conversational" ‚Üí ConversationalQAPromptWrapper(prompt_format)  
    - "longbook" ‚Üí LongContextQAPromptWrapper(prompt_format)
    - default ‚Üí MemoryPromptWrapper(prompt_format)
    ‚Üì
Each wrapper stores prompt_format for model-specific formatting
```

**Critical Relationship**: PromptWrapper takes PromptFormat in constructor and uses it to format prompts correctly for each model's tokenizer.

### 3. Complete Data Flow Diagram

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           EMOTION MEMORY EXPERIMENT                         ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              MODEL SETUP                                   ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  setup_model_and_tokenizer(repe_config)                                   ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Load Model & Tokenizer                                           ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Create PromptFormat (model-specific)                             ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ Qwen2.5PromptFormat                                          ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ LlamaPromptFormat                                            ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ OpenAIPromptFormat                                           ‚îÇ
‚îÇ      ‚îÇ   ‚îî‚îÄ‚îÄ BasePromptFormat                                             ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ Load processor (for multimodal)                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           PROMPT WRAPPER CREATION                          ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  get_memory_prompt_wrapper(task_type, prompt_format)                      ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Factory Pattern Selection:                                        ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ "passkey" ‚Üí PasskeyPromptWrapper                             ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ "conversational" ‚Üí ConversationalQAPromptWrapper             ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ "longbook" ‚Üí LongContextQAPromptWrapper                      ‚îÇ
‚îÇ      ‚îÇ   ‚îî‚îÄ‚îÄ default ‚Üí MemoryPromptWrapper                                ‚îÇ
‚îÇ      ‚îÇ                                                                     ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ Each wrapper.__init__(prompt_format)                             ‚îÇ
‚îÇ          Stores model-specific formatting logic                            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            ADAPTER & DATASET                               ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  get_adapter(config.benchmark)                                            ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ InfiniteBenchAdapter ‚Üí InfiniteBenchDataset                      ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ LongBenchAdapter ‚Üí LongBenchDataset                              ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ LoCoMoAdapter ‚Üí LoCoMoDataset                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  adapter.get_dataloader(prompt_wrapper=wrapper_partial)                   ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ Dataset receives prompt_wrapper as parameter                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                            DATASET.__GETITEM__                             ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  For each item in dataset:                                                 ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Load BenchmarkItem(id, input_text, context, ground_truth)        ‚îÇ
‚îÇ      ‚îÇ                                                                     ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ Call prompt_wrapper(context=context, question=input_text)        ‚îÇ
‚îÇ      ‚îÇ   ‚îÇ                                                                 ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ wrapper.system_prompt(context, question)                     ‚îÇ
‚îÇ      ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Task-specific system message                             ‚îÇ
‚îÇ      ‚îÇ   ‚îÇ                                                                 ‚îÇ
‚îÇ      ‚îÇ   ‚îú‚îÄ‚îÄ wrapper.user_messages(user_messages)                         ‚îÇ
‚îÇ      ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Process user instructions                                ‚îÇ
‚îÇ      ‚îÇ   ‚îÇ                                                                 ‚îÇ
‚îÇ      ‚îÇ   ‚îî‚îÄ‚îÄ prompt_format.build(system_prompt, user_messages,            ‚îÇ
‚îÇ      ‚îÇ                           enable_thinking=True/False)              ‚îÇ
‚îÇ      ‚îÇ       ‚îî‚îÄ‚îÄ üîë CRITICAL: Model-specific chat template applied        ‚îÇ
‚îÇ      ‚îÇ                                                                     ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ Return formatted dict:                                            ‚îÇ
‚îÇ          {                                                                  ‚îÇ
‚îÇ              "prompt": formatted_prompt,  # Model-ready text               ‚îÇ
‚îÇ              "item": original_item,                                         ‚îÇ
‚îÇ              "context": context,                                            ‚îÇ
‚îÇ              "question": input_text,                                        ‚îÇ
‚îÇ              "ground_truth": ground_truth,                                  ‚îÇ
‚îÇ              "metadata": metadata                                           ‚îÇ
‚îÇ          }                                                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                              DATALOADER                                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  DataLoader(dataset, batch_size=4, collate_fn=collate_memory_benchmarks)  ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ Batches formatted items for efficient processing                  ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  Batch Output:                                                              ‚îÇ
‚îÇ  {                                                                          ‚îÇ
‚îÇ      "prompt": [formatted_prompt_1, formatted_prompt_2, ...],              ‚îÇ
‚îÇ      "items": [item_1, item_2, ...],                                       ‚îÇ
‚îÇ      "contexts": [context_1, context_2, ...],                              ‚îÇ
‚îÇ      "questions": [question_1, question_2, ...],                           ‚îÇ
‚îÇ      "ground_truths": [gt_1, gt_2, ...],                                   ‚îÇ
‚îÇ      "metadata": [meta_1, meta_2, ...]                                     ‚îÇ
‚îÇ  }                                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                          EMOTION MANIPULATION                              ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  rep_control_pipeline(                                                     ‚îÇ
‚îÇ      batch["prompt"],  # ‚Üê Model-ready prompts with correct chat template ‚îÇ
‚îÇ      activations=emotion_activations,                                      ‚îÇ
‚îÇ      temperature=0.1,                                                      ‚îÇ
‚îÇ      max_new_tokens=50                                                     ‚îÇ
‚îÇ  )                                                                          ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ Generates emotionally-influenced responses                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                       ‚îÇ
                                       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                             EVALUATION                                     ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  adapter.evaluate_response(response, ground_truth, task_type)              ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ InfiniteBench: Integer extraction, F1 score                      ‚îÇ
‚îÇ      ‚îú‚îÄ‚îÄ LongBench: QA F1 score with normalization                        ‚îÇ
‚îÇ      ‚îî‚îÄ‚îÄ LoCoMo: F1 score with stemming                                    ‚îÇ
‚îÇ                                                                             ‚îÇ
‚îÇ  ResultRecord(                                                              ‚îÇ
‚îÇ      emotion=emotion,                                                       ‚îÇ
‚îÇ      intensity=intensity,                                                   ‚îÇ
‚îÇ      prompt=formatted_prompt,                                               ‚îÇ
‚îÇ      response=model_response,                                               ‚îÇ
‚îÇ      score=evaluation_score                                                 ‚îÇ
‚îÇ  )                                                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## Critical PromptFormat ‚Üî PromptWrapper Relationship

### PromptFormat Responsibilities

```python
class PromptFormat(ABC):
    """Abstract base for model-specific prompt formatting"""
    
    @abstractmethod  
    def build(self, system_prompt: str, user_messages: List[str], 
              enable_thinking: bool = False) -> str:
        """Apply model's chat template and tokenization rules"""
        pass
```

**Examples:**
- **Qwen2.5**: `<|system|>\n{system}<|user|>\n{user}<|assistant|>\n`
- **Llama**: `<s>[INST] <<SYS>>\n{system}\n<</SYS>>\n{user} [/INST]`
- **OpenAI**: `[{"role": "system", "content": "{system}"}, {"role": "user", "content": "{user}"}]`

### PromptWrapper Responsibilities

```python
class MemoryPromptWrapper(PromptWrapper):
    """Task-specific prompt content creation"""
    
    def __init__(self, prompt_format: PromptFormat):
        self.prompt_format = prompt_format  # Store model formatter
    
    def system_prompt(self, context, question):
        """Create task-specific system message content"""
        return f"Answer this question based on context: {context}\nQuestion: {question}"
    
    def __call__(self, context, question, user_messages, enable_thinking=False):
        """Combine task content with model formatting"""
        return self.prompt_format.build(  # ‚Üê Use model's chat template
            self.system_prompt(context, question),
            self.user_messages(user_messages), 
            enable_thinking=enable_thinking
        )
```

## Example: Complete Flow for Single Item

### Input Data
```json
{
  "id": "passkey_1",
  "input": "What is the pass key?", 
  "context": "The pass key is 71432. Remember it...",
  "answer": "71432"
}
```

### Step 1: Model Setup
```python
# Creates Qwen2.5PromptFormat for this model
model, tokenizer, prompt_format, processor = setup_model_and_tokenizer(repe_config)
```

### Step 2: Wrapper Creation  
```python
# Creates PasskeyPromptWrapper with Qwen2.5PromptFormat
wrapper = get_memory_prompt_wrapper("passkey", prompt_format)
```

### Step 3: Dataset Processing
```python
# Dataset.__getitem__() calls:
prompt = wrapper(
    context="The pass key is 71432. Remember it...",
    question="What is the pass key?",
    user_messages="Please provide your answer.",
    enable_thinking=False
)
```

### Step 4: Wrapper Processing
```python
# PasskeyPromptWrapper.system_prompt():
system = "Find and return the passkey mentioned in the following text.\nText: The pass key is 71432..."

# PromptFormat.build() with Qwen2.5 template:
formatted_prompt = """<|system|>
Find and return the passkey mentioned in the following text.
Text: The pass key is 71432. Remember it...
Question: What is the pass key?

<|user|>
Please provide your answer.

<|assistant|>
"""
```

### Step 5: Model Generation
```python
# rep_control_pipeline receives model-ready prompt
response = rep_control_pipeline([formatted_prompt], activations=emotion_vectors)
# ‚Üí "The pass key is 71432"
```

### Step 6: Evaluation
```python
# InfiniteBenchAdapter.evaluate_response()
score = adapter.evaluate_response("The pass key is 71432", "71432", "passkey")
# ‚Üí 1.0 (correct integer extraction)
```

## Key Design Benefits

### 1. **Separation of Concerns**
- **PromptFormat**: Handles model-specific tokenization and chat templates
- **PromptWrapper**: Handles task-specific content and structure

### 2. **Model Portability**  
- Same MemoryPromptWrapper works with any model
- Just swap PromptFormat for different tokenizers
- No hardcoded chat templates in task logic

### 3. **Task Extensibility**
- Add new tasks by creating new PromptWrapper subclasses
- Reuse existing PromptFormat for model compatibility
- Consistent evaluation through adapter pattern

### 4. **Proper Integration**
- Follows emotion_game_experiment.py patterns exactly
- Seamless DataLoader integration
- Pipeline acceleration support

This architecture ensures that emotion memory experiments work correctly across different models while maintaining proper task-specific prompting and evaluation.