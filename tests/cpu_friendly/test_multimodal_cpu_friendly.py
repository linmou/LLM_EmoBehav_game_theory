#!/usr/bin/env python3
"""
CPU-Friendly Multimodal Tests

These tests can run on CPU-only systems without GPU requirements.
They test the same core functionality using lightweight approaches.
"""

import pytest
import torch
import numpy as np
from PIL import Image
from unittest.mock import Mock, patch
import sys
import os
from pathlib import Path

# Add project root to path for imports
project_root = Path(__file__).parents[2]
sys.path.insert(0, str(project_root))

from tests.unit.neuro_manipulation.repe.test_environment import TestEnvironment, require_tokenizer, require_inference
from neuro_manipulation.prompt_formats import QwenVLInstFormat, ManualPromptFormat


class TestMultimodalCPUFriendly:
    """CPU-friendly multimodal tests that don't require GPU."""
    
    def test_environment_detection(self):
        """Test that environment detection works."""
        # This should always work
        mode = TestEnvironment.get_test_mode()
        assert mode in ["full_gpu", "cpu_inference", "tokenizer_only", "mock_only"]
        
        device_config = TestEnvironment.get_device_config()
        assert isinstance(device_config, dict)
        assert "device_map" in device_config
        
        print(f"‚úì Test mode detected: {mode}")
        print(f"‚úì Device config: {device_config}")
    
    def test_multimodal_input_detection_cpu(self):
        """Test multimodal input detection logic (CPU-only)."""
        from neuro_manipulation.repe.rep_reading_pipeline import RepReadingPipeline
        
        # Create minimal pipeline instance
        pipeline = RepReadingPipeline.__new__(RepReadingPipeline)
        
        # Test image detection
        sample_image = Image.new('RGB', (224, 224), color='red')
        
        # Test various input formats
        test_cases = [
            ({'images': [sample_image], 'text': 'test'}, True),
            ({'image': sample_image, 'text': 'test'}, True), 
            (['text only'], False),
            ('just a string', False),
            ([{'images': [sample_image], 'text': 'batch'}], True)
        ]
        
        for inputs, expected in test_cases:
            result = pipeline._is_multimodal_input(inputs)
            assert result == expected, f"Failed for input: {inputs}"
        
        print("‚úì Multimodal input detection working on CPU")
    
    def test_prompt_format_cpu_only(self):
        """Test prompt formatting without real tokenizers."""
        # Test QwenVL format building
        sample_image = Image.new('RGB', (224, 224), 'blue')
        
        # Test text-only
        text_only = QwenVLInstFormat.build(
            system_prompt=None,
            user_messages=["What is AI?"],
            assistant_answers=[]
        )
        assert '<|im_start|>' in text_only
        assert '<|im_end|>' in text_only
        assert 'What is AI?' in text_only
        
        # Test multimodal
        multimodal = QwenVLInstFormat.build(
            system_prompt=None,
            user_messages=["when you see this image, your emotion is happiness"],
            assistant_answers=[],
            images=[sample_image]
        )
        assert '<|vision_start|>' in multimodal
        assert '<|image_pad|>' in multimodal
        assert '<|vision_end|>' in multimodal
        assert 'happiness' in multimodal
        
        print("‚úì Prompt formatting working without GPU")
    
    def test_model_layer_detection_patterns(self):
        """Test model layer detection using patterns (no real models)."""
        from neuro_manipulation.model_layer_detector import ModelLayerDetector
        
        # Test name-based detection
        test_cases = [
            ("Qwen2.5-VL-3B-Instruct", True),
            ("Qwen-VL-Chat", True), 
            ("LLaVA-1.5-7B", True),
            ("GPT-4", False),
            ("Llama-2-7B", False)
        ]
        
        for model_name, expected_multimodal in test_cases:
            # Create mock model with name and empty named_modules
            mock_model = Mock()
            mock_model.__class__.__name__ = model_name
            mock_model.named_modules.return_value = []  # Empty iterator for named_modules
            
            result = ModelLayerDetector.is_multimodal_model(mock_model)
            # We expect some pattern matching to work
            print(f"Model {model_name}: multimodal={result}")
        
        print("‚úì Model pattern detection working")
    
    @require_tokenizer
    def test_tokenizer_integration_cpu(self):
        """Test tokenizer integration if available."""
        try:
            from transformers import AutoTokenizer
            
            # Try to load tokenizer
            model_path = TestEnvironment.QWEN_VL_MODEL_PATH
            tokenizer = AutoTokenizer.from_pretrained(
                model_path,
                trust_remote_code=True,
                local_files_only=True  # CPU-friendly: use cached only
            )
            
            # Test basic tokenization
            text = "when you see this image, your emotion is anger"
            tokens = tokenizer(text, return_tensors="pt")
            
            assert 'input_ids' in tokens
            assert 'attention_mask' in tokens
            
            # Test vision token detection
            vision_tokens = ['<|vision_start|>', '<|vision_end|>', '<|image_pad|>']
            for token in vision_tokens:
                token_id = tokenizer.convert_tokens_to_ids(token)
                print(f"Token '{token}' -> ID: {token_id}")
            
            print("‚úì Tokenizer integration working on CPU")
            
        except Exception as e:
            pytest.skip(f"Could not load tokenizer: {e}")
    
    def test_configuration_validation_cpu(self):
        """Test configuration file validation (CPU-only)."""
        import yaml
        
        config_path = "/data/home/jjl7137/LLM_EmoBehav_game_theory_multimodal/config/multimodal_rep_reading_config.yaml"
        
        if not os.path.exists(config_path):
            pytest.skip("Config file not found")
        
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        
        # Validate structure
        assert 'experiment' in config
        assert 'pipeline' in config['experiment']
        assert 'emotions' in config['experiment']
        
        pipeline_config = config['experiment']['pipeline']
        assert pipeline_config['task'] == 'multimodal-rep-reading'
        assert 'rep_token' in pipeline_config
        assert 'hidden_layers' in pipeline_config
        
        emotions = config['experiment']['emotions']
        required_emotions = ['anger', 'happiness', 'sadness', 'disgust', 'fear', 'surprise']
        for emotion in required_emotions:
            assert emotion in emotions
        
        print("‚úì Configuration validation working")
    
    def test_batch_processing_logic_cpu(self):
        """Test batch processing logic without inference."""
        from neuro_manipulation.repe.rep_reading_pipeline import RepReadingPipeline
        
        pipeline = RepReadingPipeline.__new__(RepReadingPipeline)
        
        # Test batch input detection
        sample_image = Image.new('RGB', (224, 224), 'green')
        batch_inputs = [
            {'images': [sample_image], 'text': 'emotion is anger'},
            {'images': [sample_image], 'text': 'emotion is happiness'}
        ]
        
        # Should detect as multimodal batch
        is_multimodal = pipeline._is_multimodal_input(batch_inputs)
        assert is_multimodal, "Should detect batch multimodal input"
        
        print("‚úì Batch processing logic working")


class TestLightweightInference:
    """Tests that can work with lightweight models or CPU inference."""
    
    @require_inference  
    def test_cpu_inference_capability(self):
        """Test if CPU inference is possible with available resources."""
        mode = TestEnvironment.get_test_mode()
        
        if mode in ["cpu_inference", "full_gpu"]:
            # We have model files and sufficient resources
            device_config = TestEnvironment.get_device_config()
            print(f"‚úì Inference possible with config: {device_config}")
            
            # Test device configuration
            if mode == "cpu_inference":
                assert device_config["device_map"] == "cpu"
                assert device_config["torch_dtype"] == torch.float32
            else:
                assert device_config["device_map"] == "auto" 
                assert device_config["torch_dtype"] == torch.bfloat16
        else:
            pytest.skip(f"Inference not available in mode: {mode}")
    
    def test_memory_requirements_check(self):
        """Test memory requirements checking."""
        import psutil
        
        # Check available memory
        ram_gb = psutil.virtual_memory().total / (1024**3)
        print(f"Available RAM: {ram_gb:.1f}GB")
        
        # Check if sufficient for CPU inference
        sufficient_ram = TestEnvironment.has_sufficient_ram()
        print(f"Sufficient for CPU inference: {sufficient_ram}")
        
        # This test always passes, just provides info
        assert ram_gb > 0  # Basic sanity check


def run_cpu_friendly_tests():
    """Run CPU-friendly tests with environment detection."""
    print("üñ•Ô∏è  CPU-Friendly Multimodal Tests")
    print("=" * 60)
    
    # Print environment info first
    TestEnvironment.print_environment_info()
    print()
    
    # Run tests with pytest
    result = pytest.main([
        __file__,
        "-v", 
        "-s",
        "--tb=short"
    ])
    
    return result == 0


if __name__ == "__main__":
    success = run_cpu_friendly_tests()
    sys.exit(0 if success else 1)