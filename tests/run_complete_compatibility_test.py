#!/usr/bin/env python3
"""
Complete compatibility test script that handles server initialization and testing.

This script:
1. Starts the OpenAI server if not already running
2. Waits for server to be ready
3. Runs comprehensive compatibility tests
4. Handles LangGraph message format conversion issues
5. Provides detailed results and recommendations

Usage:
    python run_complete_compatibility_test.py --model_path /path/to/model --model_name qwen2.5-0.5B-anger --emotion anger
"""

import argparse
import asyncio
import json
import logging
import os
import psutil
import signal
import subprocess
import sys
import time
import traceback
from pathlib import Path
from typing import Dict, List, Optional, Any
from dataclasses import dataclass

import requests
from openai import OpenAI

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class TestResult:
    """Container for test results"""
    test_name: str
    success: bool
    details: str
    execution_time: float
    error: Optional[str] = None

class ServerManager:
    """Manages the OpenAI server lifecycle"""
    
    def __init__(self, model_path: str, model_name: str, emotion: str = "anger", 
                 host: str = "localhost", port: int = 8000):
        self.model_path = model_path
        self.model_name = model_name
        self.emotion = emotion
        self.host = host
        self.port = port
        self.server_url = f"http://{host}:{port}"
        self.server_process = None
        self.server_pid = None
        self._cleanup_in_progress = False  # Prevent recursive cleanup
        
        # Register signal handlers for cleanup
        signal.signal(signal.SIGINT, self._signal_handler)
        signal.signal(signal.SIGTERM, self._signal_handler)
    
    def _signal_handler(self, signum, frame):
        """Handle signals to ensure server cleanup"""
        if self._cleanup_in_progress:
            logger.debug(f"Cleanup already in progress, ignoring signal {signum}")
            return
            
        self._cleanup_in_progress = True
        logger.info(f"Received signal {signum}, cleaning up server...")
        self.stop_server()
        sys.exit(0)
        
    def is_server_running(self) -> bool:
        """Check if server is already running"""
        try:
            response = requests.get(f"{self.server_url}/health", timeout=5)
            if response.status_code == 200:
                health_data = response.json()
                return health_data.get("status") == "healthy" and health_data.get("model_loaded")
            return False
        except Exception:
            return False
    
    def start_server(self) -> bool:
        """Start the OpenAI server"""
        if self.is_server_running():
            logger.info("‚úÖ Server is already running and ready")
            return True
        
        logger.info("üöÄ Starting OpenAI server...")
        
        # Prepare server command
        cmd = [
            sys.executable, "init_openai_server.py",
            "--model", self.model_path,
            "--model_name", self.model_name,
            "--emotion", self.emotion,
            "--host", self.host,
            "--port", str(self.port)
        ]
        
        logger.info(f"Command: {' '.join(cmd)}")
        
        try:
            # Start server process
            self.server_process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                preexec_fn=os.setsid  # Create new process group for proper cleanup
            )
            
            # Store PID for tracking
            self.server_pid = self.server_process.pid
            logger.info(f"Server started with PID: {self.server_pid}")
            
            # Wait for server to initialize
            logger.info("‚è≥ Waiting for server to initialize...")
            max_wait_time = 120  # 2 minutes
            start_time = time.time()
            
            while time.time() - start_time < max_wait_time:
                if self.server_process.poll() is not None:
                    # Process has terminated
                    stdout, stderr = self.server_process.communicate()
                    logger.error(f"‚ùå Server process terminated unexpectedly")
                    logger.error(f"STDOUT: {stdout}")
                    logger.error(f"STDERR: {stderr}")
                    return False
                
                if self.is_server_running():
                    logger.info("‚úÖ Server is ready!")
                    return True
                
                time.sleep(2)
            
            logger.error("‚ùå Server failed to start within timeout period")
            self.stop_server()
            return False
            
        except Exception as e:
            logger.error(f"‚ùå Failed to start server: {str(e)}")
            return False
    
    def find_server_processes(self) -> List[psutil.Process]:
        """Find all server processes by command line pattern"""
        server_processes = []
        current_pid = os.getpid()  # Avoid killing ourselves
        
        try:
            for proc in psutil.process_iter(['pid', 'name', 'cmdline']):
                try:
                    # Skip our own process and any test/compatibility scripts
                    if proc.info['pid'] == current_pid:
                        continue
                        
                    cmdline = ' '.join(proc.info['cmdline']) if proc.info['cmdline'] else ''
                    
                    # Only match actual server processes, not test scripts
                    if ('init_openai_server.py' in cmdline and 
                        'run_complete_compatibility_test.py' not in cmdline and
                        'test_' not in cmdline):
                        server_processes.append(proc)
                        
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    continue
        except Exception as e:
            logger.debug(f"Error finding server processes: {e}")
        
        return server_processes
    
    def kill_process_tree(self, process):
        """Kill process and all its children"""
        try:
            # Safety check: don't kill ourselves
            if process.pid == os.getpid():
                logger.warning(f"Attempted to kill own process {process.pid}, skipping")
                return
                
            # Get all child processes
            children = process.children(recursive=True)
            
            # Terminate children first
            for child in children:
                try:
                    logger.info(f"Terminating child process {child.pid}")
                    child.terminate()
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            # Wait for children to terminate
            gone, alive = psutil.wait_procs(children, timeout=5)
            
            # Force kill any remaining children
            for child in alive:
                try:
                    logger.info(f"Force killing child process {child.pid}")
                    child.kill()
                except (psutil.NoSuchProcess, psutil.AccessDenied):
                    pass
            
            # Now terminate the main process
            logger.info(f"Terminating main process {process.pid}")
            process.terminate()
            
            # Wait for main process
            try:
                process.wait(timeout=10)
            except psutil.TimeoutExpired:
                logger.info(f"Force killing main process {process.pid}")
                process.kill()
                process.wait(timeout=5)
                
        except (psutil.NoSuchProcess, psutil.AccessDenied) as e:
            logger.debug(f"Process already gone: {e}")
        except Exception as e:
            logger.error(f"Error killing process tree: {e}")

    def stop_server(self):
        """Stop the OpenAI server completely"""
        # Prevent multiple cleanup attempts
        if self._cleanup_in_progress:
            logger.debug("Server cleanup already in progress, skipping")
            return
            
        self._cleanup_in_progress = True
        logger.info("üõë Stopping server...")
        
        # Method 1: Stop the tracked process
        if self.server_process:
            try:
                # Convert to psutil process for better control
                proc = psutil.Process(self.server_process.pid)
                self.kill_process_tree(proc)
                logger.info("‚úÖ Tracked server process stopped")
            except (psutil.NoSuchProcess, psutil.AccessDenied):
                logger.info("Tracked process already gone")
            except Exception as e:
                logger.error(f"Error stopping tracked process: {e}")
            finally:
                self.server_process = None
                self.server_pid = None
        
        # Method 2: Find and kill any remaining server processes
        server_processes = self.find_server_processes()
        if server_processes:
            logger.info(f"Found {len(server_processes)} additional server processes to stop")
            for proc in server_processes:
                try:
                    # Double check it's not our own process before killing
                    if proc.pid != os.getpid():
                        logger.info(f"Stopping server process {proc.pid}: {' '.join(proc.cmdline())}")
                        self.kill_process_tree(proc)
                    else:
                        logger.debug(f"Skipping own process {proc.pid}")
                except Exception as e:
                    logger.error(f"Error stopping process {proc.pid}: {e}")
        
        # Method 3: Check port and kill process using port (last resort)
        try:
            import shutil
            if shutil.which('lsof'):
                result = subprocess.run(
                    ['lsof', '-ti', f':{self.port}'], 
                    capture_output=True, text=True, timeout=5
                )
                if result.returncode == 0 and result.stdout.strip():
                    pids = result.stdout.strip().split('\n')
                    for pid in pids:
                        try:
                            pid = int(pid)
                            proc = psutil.Process(pid)
                            logger.info(f"Killing process using port {self.port}: PID {pid}")
                            self.kill_process_tree(proc)
                        except Exception as e:
                            logger.debug(f"Error killing process {pid}: {e}")
        except Exception as e:
            logger.debug(f"Could not check port usage: {e}")
        
        # Verify server is stopped
        time.sleep(2)
        if not self.is_server_running():
            logger.info("‚úÖ Server completely stopped")
        else:
            logger.warning("‚ö†Ô∏è Server may still be running")
            
        # Reset cleanup flag
        self._cleanup_in_progress = False
    
    def __del__(self):
        """Cleanup on deletion"""
        try:
            self.stop_server()
        except Exception:
            pass

class EnhancedCompatibilityTester:
    """Enhanced test class with LangGraph fixes"""
    
    def __init__(self, server_url: str, model_name: str, api_key: str = "token-abc123"):
        self.server_url = server_url.rstrip('/')
        self.base_url = f"{server_url}/v1"
        self.model_name = model_name
        self.api_key = api_key
        self.client = OpenAI(base_url=self.base_url, api_key=api_key)
        self.test_results: List[TestResult] = []
        
    def log_test_result(self, result: TestResult):
        """Log and store test result"""
        self.test_results.append(result)
        status = "‚úÖ PASS" if result.success else "‚ùå FAIL"
        logger.info(f"{status} - {result.test_name} ({result.execution_time:.2f}s)")
        if result.details:
            logger.info(f"  Details: {result.details}")
        if result.error:
            logger.debug(f"  Error: {result.error}")

    def run_test(self, test_name: str, test_func, *args, **kwargs) -> TestResult:
        """Run a single test with timing and error handling"""
        start_time = time.time()
        try:
            success, details = test_func(*args, **kwargs)
            execution_time = time.time() - start_time
            result = TestResult(test_name, success, details, execution_time)
        except Exception as e:
            execution_time = time.time() - start_time
            result = TestResult(
                test_name, 
                False, 
                f"Exception occurred: {str(e)}", 
                execution_time,
                traceback.format_exc()
            )
        
        self.log_test_result(result)
        return result

    # ========== Basic Tests ==========
    
    def test_server_health(self) -> tuple[bool, str]:
        """Test server health endpoint"""
        try:
            response = requests.get(f"{self.server_url}/health", timeout=10)
            if response.status_code == 200:
                health_data = response.json()
                if health_data.get("status") == "healthy" and health_data.get("model_loaded"):
                    return True, f"Server healthy: {health_data}"
                else:
                    return False, f"Server not ready: {health_data}"
            else:
                return False, f"Health check failed: {response.status_code}"
        except Exception as e:
            return False, f"Health check error: {str(e)}"

    def test_basic_chat_completion(self) -> tuple[bool, str]:
        """Test basic chat completion"""
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "Hello! Please respond briefly."}],
                max_tokens=50,
                temperature=0.0
            )
            
            if response.choices and response.choices[0].message.content:
                content = response.choices[0].message.content.strip()
                usage = response.usage
                return True, f"Response: '{content[:100]}...' | Usage: {usage}"
            else:
                return False, "Empty response from chat completion"
        except Exception as e:
            return False, f"Chat completion error: {str(e)}"

    # ========== Enhanced LangGraph Tests ==========
    
    def test_langgraph_basic_fixed(self) -> tuple[bool, str]:
        """Test LangGraph with proper message conversion"""
        try:
            from langgraph.graph import StateGraph
            from langgraph.graph.message import add_messages
            from typing_extensions import TypedDict
            from typing import Annotated
            from langchain_core.messages import HumanMessage, AIMessage
            
            # Define state
            class State(TypedDict):
                messages: Annotated[list, add_messages]
            
            # Create a node function with proper message conversion
            def chatbot_node(state: State):
                # Convert LangChain messages to OpenAI format
                openai_messages = []
                for msg in state["messages"]:
                    if hasattr(msg, 'content'):
                        if isinstance(msg, HumanMessage):
                            openai_messages.append({"role": "user", "content": msg.content})
                        elif isinstance(msg, AIMessage):
                            openai_messages.append({"role": "assistant", "content": msg.content})
                        else:
                            # Handle dict messages
                            openai_messages.append(msg)
                    else:
                        openai_messages.append(msg)
                
                # Get response from our server
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=openai_messages,
                    max_tokens=30,
                    temperature=0.0
                )
                
                # Convert back to LangChain message format
                content = response.choices[0].message.content
                ai_message = AIMessage(content=content)
                
                return {"messages": [ai_message]}
            
            # Build graph
            graph_builder = StateGraph(State)
            graph_builder.add_node("chatbot", chatbot_node)
            graph_builder.set_entry_point("chatbot")
            graph_builder.set_finish_point("chatbot")
            graph = graph_builder.compile()
            
            # Test the graph with proper message format
            initial_message = HumanMessage(content="Say hello briefly")
            result = graph.invoke({"messages": [initial_message]})
            
            if result and "messages" in result and len(result["messages"]) > 1:
                response_content = result["messages"][-1].content
                return True, f"LangGraph working: '{response_content[:100]}...'"
            else:
                return False, f"LangGraph failed to produce expected result: {result}"
                
        except ImportError as e:
            return False, f"LangGraph not installed: {str(e)}"
        except Exception as e:
            return False, f"LangGraph test error: {str(e)}"

    def test_langgraph_with_tools_fixed(self) -> tuple[bool, str]:
        """Test LangGraph with tools and proper message handling"""
        try:
            from langgraph.graph import StateGraph
            from langgraph.graph.message import add_messages
            from langgraph.prebuilt import ToolNode, tools_condition
            from typing_extensions import TypedDict
            from typing import Annotated
            from langchain_core.tools import tool
            from langchain_core.messages import HumanMessage, AIMessage, ToolMessage
            
            # Define a simple tool
            @tool
            def get_weather(location: str) -> str:
                """Get weather for a location"""
                return f"Sunny, 75¬∞F in {location}"
            
            tools = [get_weather]
            
            # Define state
            class State(TypedDict):
                messages: Annotated[list, add_messages]
            
            # Create chatbot node with proper message conversion
            def chatbot_node(state: State):
                # Convert messages to OpenAI format
                openai_messages = []
                for msg in state["messages"]:
                    if hasattr(msg, 'content') and hasattr(msg, 'type'):
                        if msg.type == "human":
                            openai_messages.append({"role": "user", "content": msg.content})
                        elif msg.type == "ai":
                            openai_messages.append({"role": "assistant", "content": msg.content})
                        elif msg.type == "tool":
                            # Skip tool messages for this simple test
                            continue
                    else:
                        # Handle dict format
                        openai_messages.append(msg)
                
                response = self.client.chat.completions.create(
                    model=self.model_name,
                    messages=openai_messages,
                    max_tokens=50,
                    temperature=0.0
                )
                
                # Convert back to LangChain format
                content = response.choices[0].message.content
                ai_message = AIMessage(content=content)
                
                return {"messages": [ai_message]}
            
            # Build graph with tools
            graph_builder = StateGraph(State)
            graph_builder.add_node("chatbot", chatbot_node)
            
            tool_node = ToolNode(tools)
            graph_builder.add_node("tools", tool_node)
            
            # For this test, we'll skip tool calling and just test basic functionality
            graph_builder.set_entry_point("chatbot")
            graph_builder.set_finish_point("chatbot")
            
            graph = graph_builder.compile()
            
            # Test simple conversation (no tools)
            initial_message = HumanMessage(content="Hello, how are you?")
            result = graph.invoke({"messages": [initial_message]})
            
            if result and "messages" in result:
                return True, f"LangGraph with tools setup successful: {len(result['messages'])} messages"
            else:
                return False, f"LangGraph with tools failed: {result}"
                
        except ImportError as e:
            return False, f"LangGraph/LangChain dependencies not installed: {str(e)}"
        except Exception as e:
            return False, f"LangGraph tools test error: {str(e)}"

    # ========== AG2 Tests ==========
    
    def test_ag2_basic(self) -> tuple[bool, str]:
        """Test basic AG2 functionality"""
        try:
            import autogen
            
            config_list = [{
                "model": self.model_name,
                "api_key": self.api_key,
                "base_url": self.base_url,
                "api_type": "openai"
            }]
            
            assistant = autogen.AssistantAgent(
                "assistant",
                llm_config={"config_list": config_list, "cache_seed": None},
                max_consecutive_auto_reply=1
            )
            
            user_proxy = autogen.UserProxyAgent(
                "user_proxy",
                human_input_mode="NEVER",
                max_consecutive_auto_reply=1,
                is_termination_msg=lambda x: True,
                code_execution_config=False
            )
            
            chat_result = user_proxy.initiate_chat(
                assistant,
                message="Hello! Please respond briefly with just 'Hi there!'",
                max_turns=1
            )
            
            if chat_result and hasattr(chat_result, 'chat_history') and len(chat_result.chat_history) >= 2:
                assistant_response = chat_result.chat_history[-1]['content']
                return True, f"AG2 working: '{assistant_response[:100]}...'"
            else:
                return False, f"AG2 failed to produce expected chat result: {chat_result}"
                
        except ImportError as e:
            return False, f"AG2 (AutoGen) not installed: {str(e)}"
        except Exception as e:
            return False, f"AG2 test error: {str(e)}"

    # ========== Advanced Tests ==========
    
    def test_streaming_support(self) -> tuple[bool, str]:
        """Test streaming functionality"""
        try:
            stream = self.client.chat.completions.create(
                model=self.model_name,
                messages=[{"role": "user", "content": "Count to 5"}],
                max_tokens=50,
                stream=True,
                temperature=0.0
            )
            
            chunks = []
            for chunk in stream:
                if chunk.choices and chunk.choices[0].delta.content:
                    chunks.append(chunk.choices[0].delta.content)
                if len(chunks) >= 5:  # Limit for testing
                    break
            
            if chunks:
                content = ''.join(chunks)
                return True, f"Streaming working: {len(chunks)} chunks, content: '{content[:50]}...'"
            else:
                return False, "No streaming chunks received"
                
        except Exception as e:
            return False, f"Streaming test error: {str(e)}"

    def run_all_tests(self) -> Dict[str, Any]:
        """Run all compatibility tests"""
        logger.info("=" * 60)
        logger.info("ENHANCED LANGGRAPH & AG2 COMPATIBILITY TESTS")
        logger.info("=" * 60)
        logger.info(f"Server URL: {self.server_url}")
        logger.info(f"Model: {self.model_name}")
        logger.info("")
        
        # Basic tests
        logger.info("üîç Basic OpenAI Compatibility Tests")
        logger.info("-" * 40)
        self.run_test("Server Health Check", self.test_server_health)
        self.run_test("Basic Chat Completion", self.test_basic_chat_completion)
        
        # Advanced tests
        logger.info("\nüöÄ Advanced Feature Tests")
        logger.info("-" * 40)
        self.run_test("Streaming Support", self.test_streaming_support)
        
        # LangGraph tests (fixed)
        logger.info("\nüîó LangGraph Compatibility Tests (Enhanced)")
        logger.info("-" * 40)
        self.run_test("LangGraph Basic Functionality (Fixed)", self.test_langgraph_basic_fixed)
        self.run_test("LangGraph with Tools (Fixed)", self.test_langgraph_with_tools_fixed)
        
        # AG2 tests
        logger.info("\nü§ñ AG2 (AutoGen) Compatibility Tests")
        logger.info("-" * 40)
        self.run_test("AG2 Basic Functionality", self.test_ag2_basic)
        
        return self.generate_summary()

    def generate_summary(self) -> Dict[str, Any]:
        """Generate test summary"""
        total_tests = len(self.test_results)
        passed_tests = sum(1 for result in self.test_results if result.success)
        failed_tests = total_tests - passed_tests
        
        summary = {
            "total_tests": total_tests,
            "passed_tests": passed_tests,
            "failed_tests": failed_tests,
            "success_rate": passed_tests / total_tests if total_tests > 0 else 0
        }
        
        logger.info("\n" + "=" * 60)
        logger.info("üìä TEST SUMMARY")
        logger.info("=" * 60)
        logger.info(f"Overall: {passed_tests}/{total_tests} tests passed ({summary['success_rate']:.1%})")
        
        logger.info("\nüìã Detailed Results:")
        for result in self.test_results:
            status = "‚úÖ" if result.success else "‚ùå"
            logger.info(f"  {status} {result.test_name}")
            if not result.success and result.error:
                logger.info(f"    Error: {result.details}")
        
        logger.info("\nüí° Recommendations:")
        if summary['success_rate'] == 1.0:
            logger.info("  üéâ All tests passed! Your server is fully compatible.")
        elif summary['success_rate'] > 0.8:
            logger.info("  ‚úÖ Most tests passed. Your server has excellent compatibility.")
        else:
            logger.info("  ‚ö†Ô∏è  Some tests failed. Check the errors above for details.")
        
        return summary

def main():
    """Main function"""
    parser = argparse.ArgumentParser(description="Complete OpenAI server compatibility test")
    parser.add_argument("--model_path", required=True, help="Path to the model")
    parser.add_argument("--model_name", required=True, help="Model name")
    parser.add_argument("--emotion", default="anger", help="Emotion setting")
    parser.add_argument("--host", default="localhost", help="Server host")
    parser.add_argument("--port", type=int, default=8000, help="Server port")
    parser.add_argument("--api_key", default="token-abc123", help="API key")
    parser.add_argument("--no_server", action="store_true", help="Skip server startup (assume already running)")
    parser.add_argument("--output", help="JSON output file for results")
    parser.add_argument("--keep_server", action="store_true", help="Don't stop server after tests (for debugging)")
    
    args = parser.parse_args()
    
    server_manager = None
    exit_code = 0
    
    def cleanup_and_exit(code=0):
        """Cleanup function with proper server shutdown"""
        nonlocal server_manager
        if server_manager and not args.keep_server:
            try:
                logger.info("\nüßπ Cleaning up server...")
                server_manager.stop_server()
                logger.info("‚úÖ Cleanup completed")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Error during cleanup: {e}")
        elif args.keep_server:
            logger.info("\nüîÑ Keeping server running (--keep_server flag)")
        sys.exit(code)
    
    try:
        # Initialize server if needed
        if not args.no_server:
            logger.info("üöÄ Initializing server manager...")
            server_manager = ServerManager(
                args.model_path, args.model_name, args.emotion, args.host, args.port
            )
            
            logger.info("‚è≥ Starting server...")
            if not server_manager.start_server():
                logger.error("‚ùå Failed to start server. Exiting.")
                cleanup_and_exit(1)
        
        # Run tests
        logger.info("üß™ Starting compatibility tests...")
        server_url = f"http://{args.host}:{args.port}"
        tester = EnhancedCompatibilityTester(server_url, args.model_name, args.api_key)
        summary = tester.run_all_tests()
        
        # Save results if requested
        if args.output:
            try:
                with open(args.output, 'w') as f:
                    json.dump({
                        "summary": summary,
                        "detailed_results": [
                            {
                                "test_name": r.test_name,
                                "success": r.success,
                                "details": r.details,
                                "execution_time": r.execution_time,
                                "error": r.error
                            }
                            for r in tester.test_results
                        ]
                    }, f, indent=2)
                logger.info(f"\nüìÑ Results saved to: {args.output}")
            except Exception as e:
                logger.error(f"‚ö†Ô∏è Failed to save results: {e}")
        
        # Determine exit code
        if summary["success_rate"] >= 0.8:
            logger.info("\nüéâ Tests completed successfully!")
            exit_code = 0
        else:
            logger.error("\n‚ùå Some tests failed.")
            exit_code = 1
        
        cleanup_and_exit(exit_code)
    
    except KeyboardInterrupt:
        logger.info("\n‚ö†Ô∏è Test interrupted by user (Ctrl+C)")
        logger.info("üßπ Performing cleanup...")
        cleanup_and_exit(130)  # Standard exit code for SIGINT
    
    except Exception as e:
        logger.error(f"\n‚ùå Unexpected error: {str(e)}")
        logger.error(f"Error details: {traceback.format_exc()}")
        cleanup_and_exit(1)
    
    finally:
        # Fallback cleanup (should not be reached due to cleanup_and_exit calls)
        if server_manager and not args.keep_server:
            try:
                server_manager.stop_server()
            except Exception:
                pass

if __name__ == "__main__":
    main() 