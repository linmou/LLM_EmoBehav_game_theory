# Full TrustLLM Ethics run (implicit_ethics, social_norm, explicit_low/high)
# Edit the data_path values to point to your local TrustLLM ethics JSON files.
# Typical filenames (from TrustLLM docs):
#   implicit_ETHICS.json
#   implicit_SocialChemistry101.json
#   explicit_moralchoice.json

models:
  - /data/home/jjl7137/huggingface_models/Qwen/Qwen3-0.6B
  - /data/home/jjl7137/huggingface_models/Qwen/Qwen3-1.7B
  - /data/home/jjl7137/huggingface_models/Qwen/Qwen3-4B

benchmarks:
  - name: trustllm_ethics
    task_type: implicit_ethics
    data_path: /data/home/jjl7137/TrustLLM/data/TrustLLM/dataset/ethics/implicit_ETHICS.json
    enable_auto_truncation: false
    truncation_strategy: right
    preserve_ratio: 0.8
    llm_eval_config:
      model: gpt-4o-mini
      temperature: 0.0

  - name: trustllm_ethics
    task_type: social_norm
    data_path: /data/home/jjl7137/TrustLLM/data/TrustLLM/dataset/ethics/implicit_SocialChemistry101.json
    enable_auto_truncation: false
    truncation_strategy: right
    preserve_ratio: 0.8
    llm_eval_config:
      model: gpt-4o-mini
      temperature: 0.0

  - name: trustllm_ethics
    task_type: explicit_low
    data_path: /data/home/jjl7137/TrustLLM/data/TrustLLM/dataset/ethics/explicit_moralchoice.json
    enable_auto_truncation: false
    truncation_strategy: right
    preserve_ratio: 0.8
    llm_eval_config:
      model: gpt-4o-mini
      temperature: 0.0

  - name: trustllm_ethics
    task_type: explicit_high
    data_path: /data/home/jjl7137/TrustLLM/data/TrustLLM/dataset/ethics/explicit_moralchoice.json
    enable_auto_truncation: false
    truncation_strategy: right
    preserve_ratio: 0.8
    llm_eval_config:
      model: gpt-4o-mini
      temperature: 0.0

emotions: ["anger", "happiness", "sadness", "fear", "disgust", "surprise"]
intensities: [1.5]

repeat_runs: 3
repeat_seed_base: 123

output_dir: results/TrustLLM_ethics/thinking
batch_size: 800

sanity_check: false

generation_config:
  temperature: 0.0
  max_new_tokens: 3000
  do_sample: false
  enable_thinking: true

loading_config:
  gpu_memory_utilization: 0.90
  max_model_len: 8192
  enforce_eager: true
  trust_remote_code: true
  dtype: float16
  seed: 42
  disable_custom_all_reduce: false
  additional_vllm_kwargs: {}

